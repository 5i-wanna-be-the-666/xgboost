{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/q_/hdw234vx4f39sz54z07zrvt40000gn/T/ipykernel_8457/1269278097.py:79: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if ' positive' in y:\n",
      "[I 2025-05-07 14:36:35,225] A new study created in memory with name: no-name-99184cea-d76b-4f01-ab2e-57243b1ae59a\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function train at 0x13e717c70>\n",
      "       Mcg   Gvh   Alm   Mit  Erl  Pox   Vac   Nuc  Class\n",
      "0     0.58  0.61  0.47  0.13  0.5  0.0  0.48  0.22      0\n",
      "1     0.43  0.67  0.48  0.27  0.5  0.0  0.53  0.22      0\n",
      "2     0.64  0.62  0.49  0.15  0.5  0.0  0.53  0.22      0\n",
      "3     0.58  0.44  0.57  0.13  0.5  0.0  0.54  0.22      0\n",
      "4     0.42  0.44  0.48  0.54  0.5  0.0  0.48  0.22      0\n",
      "...    ...   ...   ...   ...  ...  ...   ...   ...    ...\n",
      "1479  0.81  0.62  0.43  0.17  0.5  0.0  0.53  0.22      0\n",
      "1480  0.47  0.43  0.61  0.40  0.5  0.0  0.48  0.47      0\n",
      "1481  0.67  0.57  0.36  0.19  0.5  0.0  0.56  0.22      0\n",
      "1482  0.43  0.40  0.60  0.16  0.5  0.0  0.53  0.39      0\n",
      "1483  0.65  0.54  0.54  0.13  0.5  0.0  0.53  0.22      0\n",
      "\n",
      "[1484 rows x 9 columns]\n",
      "(1484, 8)\n",
      "[0 0 0 ... 0 0 0]\n",
      "üîç Ê≠£Âú®ÊâßË°åÂÖ®Â±ÄË∂ÖÂèÇÊï∞ÊêúÁ¥¢Ôºà‰ΩøÁî®ËÆ≠ÁªÉÈõÜÔºâ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 14:36:35,550] Trial 0 finished with values: [0.9502463054187191, 0.844862771900765] and parameters: {'n_estimators': 417, 'max_depth': 11, 'learning_rate': 0.27076332534668524, 'subsample': 0.9961520327656137, 'colsample_bytree': 0.9159040707564969, 'gamma': 2.2971292264731256, 'min_child_weight': 9, 'scale_pos_weight': 2.8174218757835394}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:35,999] Trial 1 finished with values: [0.9433497536945812, 0.6158817620063862] and parameters: {'n_estimators': 382, 'max_depth': 7, 'learning_rate': 0.012164688152637083, 'subsample': 0.6781313276975014, 'colsample_bytree': 0.6620614933380435, 'gamma': 0.32171153448682044, 'min_child_weight': 10, 'scale_pos_weight': 7.284188227595061}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:36,309] Trial 2 finished with values: [0.8665024630541872, 0.7327658648772634] and parameters: {'n_estimators': 453, 'max_depth': 10, 'learning_rate': 0.07669028557701267, 'subsample': 0.695324812590246, 'colsample_bytree': 0.9625072113379032, 'gamma': 0.2639937240097934, 'min_child_weight': 10, 'scale_pos_weight': 7.166460058072015}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:36,366] Trial 3 finished with values: [0.9620689655172414, 0.0] and parameters: {'n_estimators': 99, 'max_depth': 3, 'learning_rate': 0.015708853818734637, 'subsample': 0.7367302515740781, 'colsample_bytree': 0.8126721265800612, 'gamma': 0.0601101257068376, 'min_child_weight': 7, 'scale_pos_weight': 0.6635697487439884}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:36,564] Trial 4 finished with values: [0.9669950738916255, 0.9413856257783348] and parameters: {'n_estimators': 437, 'max_depth': 9, 'learning_rate': 0.04139699245119708, 'subsample': 0.9878175635778628, 'colsample_bytree': 0.5690907855548613, 'gamma': 1.333716052814569, 'min_child_weight': 6, 'scale_pos_weight': 0.9946986946297245}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:36,888] Trial 5 finished with values: [0.9211822660098522, 0.7552770019471238] and parameters: {'n_estimators': 404, 'max_depth': 12, 'learning_rate': 0.01435482086468057, 'subsample': 0.8988484141981292, 'colsample_bytree': 0.6390388097840833, 'gamma': 2.5207799660237113, 'min_child_weight': 3, 'scale_pos_weight': 5.173387488803784}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:36,930] Trial 6 finished with values: [0.9423645320197044, 0.0] and parameters: {'n_estimators': 52, 'max_depth': 4, 'learning_rate': 0.031255636962502595, 'subsample': 0.9455598362778762, 'colsample_bytree': 0.5040250299022451, 'gamma': 2.6507779929249864, 'min_child_weight': 9, 'scale_pos_weight': 9.840833157089161}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:37,268] Trial 7 finished with values: [0.8660098522167488, 0.8048498314199692] and parameters: {'n_estimators': 363, 'max_depth': 8, 'learning_rate': 0.03911783384197456, 'subsample': 0.6465800996623565, 'colsample_bytree': 0.6168922048553336, 'gamma': 0.04046087176716895, 'min_child_weight': 2, 'scale_pos_weight': 5.5700431556774195}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:37,300] Trial 8 finished with values: [0.9322660098522167, 0.8033182413899993] and parameters: {'n_estimators': 63, 'max_depth': 12, 'learning_rate': 0.20281675416279527, 'subsample': 0.9940644809393235, 'colsample_bytree': 0.9397106884042725, 'gamma': 1.9167309212142403, 'min_child_weight': 7, 'scale_pos_weight': 0.6430865506912158}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:37,429] Trial 9 finished with values: [0.8926108374384236, 0.8351867729500716] and parameters: {'n_estimators': 212, 'max_depth': 11, 'learning_rate': 0.09534775124828505, 'subsample': 0.921669640120061, 'colsample_bytree': 0.7677800444422109, 'gamma': 2.1151616620265603, 'min_child_weight': 6, 'scale_pos_weight': 6.510139066931147}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:37,631] Trial 10 finished with values: [0.9467980295566502, 0.7812456895861315] and parameters: {'n_estimators': 428, 'max_depth': 8, 'learning_rate': 0.028079123409244167, 'subsample': 0.9696960371300161, 'colsample_bytree': 0.7900968088680039, 'gamma': 4.198792175008932, 'min_child_weight': 7, 'scale_pos_weight': 4.140415488781512}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:37,736] Trial 11 finished with values: [0.877832512315271, 0.7344445975811623] and parameters: {'n_estimators': 258, 'max_depth': 10, 'learning_rate': 0.2087277832564792, 'subsample': 0.8386430529802191, 'colsample_bytree': 0.6350254059603473, 'gamma': 4.715134524089683, 'min_child_weight': 7, 'scale_pos_weight': 5.534777416138276}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:37,836] Trial 12 finished with values: [0.9694581280788177, 0.0] and parameters: {'n_estimators': 180, 'max_depth': 11, 'learning_rate': 0.011653999420831437, 'subsample': 0.8401358343218621, 'colsample_bytree': 0.7643971206067106, 'gamma': 2.828750071259394, 'min_child_weight': 10, 'scale_pos_weight': 3.41722423003412}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:38,010] Trial 13 finished with values: [0.8704433497536946, 0.7847060256605273] and parameters: {'n_estimators': 459, 'max_depth': 5, 'learning_rate': 0.06163640418497415, 'subsample': 0.5517214523515495, 'colsample_bytree': 0.5726354597016126, 'gamma': 3.287260254379876, 'min_child_weight': 2, 'scale_pos_weight': 3.343593284472996}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:38,187] Trial 14 finished with values: [0.9211822660098522, 0.45863365288150443] and parameters: {'n_estimators': 189, 'max_depth': 10, 'learning_rate': 0.017668986861254947, 'subsample': 0.8405924146361086, 'colsample_bytree': 0.8717451013093436, 'gamma': 2.0014026391223476, 'min_child_weight': 5, 'scale_pos_weight': 5.992153829931966}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:38,264] Trial 15 finished with values: [0.9108374384236454, 0.7630635686278229] and parameters: {'n_estimators': 194, 'max_depth': 7, 'learning_rate': 0.15234139008052325, 'subsample': 0.8821126997365383, 'colsample_bytree': 0.7021685221997219, 'gamma': 3.6482212184083895, 'min_child_weight': 8, 'scale_pos_weight': 4.139698101715602}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:38,665] Trial 16 finished with values: [0.8980295566502462, 0.8492245085396083] and parameters: {'n_estimators': 449, 'max_depth': 8, 'learning_rate': 0.01770556811459331, 'subsample': 0.9571976090633583, 'colsample_bytree': 0.9465258864873899, 'gamma': 0.0008207538271359782, 'min_child_weight': 5, 'scale_pos_weight': 8.257616801176157}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:38,768] Trial 17 finished with values: [0.8655172413793104, 0.6639929965315033] and parameters: {'n_estimators': 193, 'max_depth': 4, 'learning_rate': 0.12443444555221732, 'subsample': 0.5323319319787845, 'colsample_bytree': 0.9480070394844292, 'gamma': 4.050819997443566, 'min_child_weight': 7, 'scale_pos_weight': 4.965081277609972}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:38,926] Trial 18 finished with values: [0.9408866995073891, 0.24211709903803963] and parameters: {'n_estimators': 222, 'max_depth': 6, 'learning_rate': 0.015333740832752607, 'subsample': 0.5041484497338438, 'colsample_bytree': 0.5305622122533643, 'gamma': 2.6198620364574636, 'min_child_weight': 4, 'scale_pos_weight': 4.8433216531223735}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:39,070] Trial 19 finished with values: [0.8453201970443349, 0.8289705170427119] and parameters: {'n_estimators': 223, 'max_depth': 11, 'learning_rate': 0.2342798148370094, 'subsample': 0.7416740434995088, 'colsample_bytree': 0.686297098451927, 'gamma': 0.053156728853618884, 'min_child_weight': 1, 'scale_pos_weight': 1.6572691102744745}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:39,308] Trial 20 finished with values: [0.8901477832512316, 0.7878385971007028] and parameters: {'n_estimators': 399, 'max_depth': 10, 'learning_rate': 0.016457410356983188, 'subsample': 0.601844164904126, 'colsample_bytree': 0.9226654347956875, 'gamma': 4.727827487502518, 'min_child_weight': 1, 'scale_pos_weight': 5.53182805751471}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:39,480] Trial 21 finished with values: [0.9408866995073891, 0.7474093186289847] and parameters: {'n_estimators': 453, 'max_depth': 5, 'learning_rate': 0.020105138364338714, 'subsample': 0.5266335998953572, 'colsample_bytree': 0.8392111629316219, 'gamma': 4.486457586588526, 'min_child_weight': 10, 'scale_pos_weight': 1.8661902223500715}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:39,615] Trial 22 finished with values: [0.9517241379310345, 0.8439877195977113] and parameters: {'n_estimators': 393, 'max_depth': 10, 'learning_rate': 0.15233363368837277, 'subsample': 0.7701560790409686, 'colsample_bytree': 0.9168590369196056, 'gamma': 4.890837016838022, 'min_child_weight': 2, 'scale_pos_weight': 1.0193175264470475}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:39,726] Trial 23 finished with values: [0.9354679802955665, 0.749712588553112] and parameters: {'n_estimators': 105, 'max_depth': 9, 'learning_rate': 0.0494794106659622, 'subsample': 0.8625654302008021, 'colsample_bytree': 0.6875423040345372, 'gamma': 3.573835119178372, 'min_child_weight': 2, 'scale_pos_weight': 7.49978762226526}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:39,886] Trial 24 finished with values: [0.9586206896551724, 0.42345052225892005] and parameters: {'n_estimators': 234, 'max_depth': 3, 'learning_rate': 0.014493005900610654, 'subsample': 0.6477385523057733, 'colsample_bytree': 0.9249576003272051, 'gamma': 3.0064217977902223, 'min_child_weight': 7, 'scale_pos_weight': 3.6467141230577407}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:40,167] Trial 25 finished with values: [0.8591133004926107, 0.7878385971007028] and parameters: {'n_estimators': 372, 'max_depth': 12, 'learning_rate': 0.04581128173600758, 'subsample': 0.5767860820252835, 'colsample_bytree': 0.6036741653896205, 'gamma': 1.3255037717156315, 'min_child_weight': 3, 'scale_pos_weight': 5.673903939303986}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:40,295] Trial 26 finished with values: [0.9571428571428571, 0.0] and parameters: {'n_estimators': 166, 'max_depth': 12, 'learning_rate': 0.01717259074970716, 'subsample': 0.6026584578289729, 'colsample_bytree': 0.9196591380567289, 'gamma': 3.855652750000447, 'min_child_weight': 6, 'scale_pos_weight': 3.647509677192256}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:40,532] Trial 27 finished with values: [0.9167487684729063, 0.5901490167898367] and parameters: {'n_estimators': 276, 'max_depth': 7, 'learning_rate': 0.017067931308086617, 'subsample': 0.9117818491354344, 'colsample_bytree': 0.5579315153269127, 'gamma': 0.7991131846793131, 'min_child_weight': 10, 'scale_pos_weight': 9.774143043602582}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:40,743] Trial 28 finished with values: [0.8088669950738916, 0.7987058497278118] and parameters: {'n_estimators': 324, 'max_depth': 3, 'learning_rate': 0.08216692669572523, 'subsample': 0.8996022438818847, 'colsample_bytree': 0.752015569969691, 'gamma': 0.07846259000760913, 'min_child_weight': 5, 'scale_pos_weight': 5.98568389007075}.\n",
      "/opt/anaconda3/envs/lhy_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [14:36:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-07 14:36:41,405] Trial 29 finished with values: [0.8876847290640395, 0.7334378190440253] and parameters: {'n_estimators': 479, 'max_depth': 5, 'learning_rate': 0.028099318370741113, 'subsample': 0.545936259855684, 'colsample_bytree': 0.7228650759153208, 'gamma': 0.19448257129264235, 'min_child_weight': 9, 'scale_pos_weight': 5.4416783032192555}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Pareto ÊúÄ‰ºòËß£Êï∞Èáè: 2\n",
      "  AUC=0.9670, G-Mean=0.9414, params={'n_estimators': 437, 'max_depth': 9, 'learning_rate': 0.04139699245119708, 'subsample': 0.9878175635778628, 'colsample_bytree': 0.5690907855548613, 'gamma': 1.333716052814569, 'min_child_weight': 6, 'scale_pos_weight': 0.9946986946297245}\n",
      "  AUC=0.9695, G-Mean=0.0000, params={'n_estimators': 180, 'max_depth': 11, 'learning_rate': 0.011653999420831437, 'subsample': 0.8401358343218621, 'colsample_bytree': 0.7643971206067106, 'gamma': 2.828750071259394, 'min_child_weight': 10, 'scale_pos_weight': 3.41722423003412}\n",
      "‚úÖ ÂÖ®Â±ÄÊúÄ‰ºòÂèÇÊï∞Ôºö {'n_estimators': 437, 'max_depth': 9, 'learning_rate': 0.04139699245119708, 'subsample': 0.9878175635778628, 'colsample_bytree': 0.5690907855548613, 'gamma': 1.333716052814569, 'min_child_weight': 6, 'scale_pos_weight': 0.9946986946297245}\n",
      "\n",
      "--- Fold 1 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 1\n",
      "Training classifier 2/3 for fold 1\n",
      "Training classifier 3/3 for fold 1\n",
      "[0.00368624 0.00523295 0.00365244 0.00380244 0.0392527  0.02717197\n",
      " 0.00204688 0.00459906 0.00196968 0.00498385 0.00266654 0.00729652\n",
      " 0.00217731 0.00246884 0.00329416 0.0025026  0.00193787 0.00180384\n",
      " 0.00293575 0.00563685 0.00868298 0.45392975 0.00689562 0.00185303\n",
      " 0.01006108 0.00259335 0.00249952 0.0020742  0.00198167 0.00249952\n",
      " 0.00307132 0.00304745 0.01523291 0.00611918 0.00272986 0.00184826\n",
      " 0.01241636 0.0019164  0.0019164  0.00565804 0.00259934 0.46068734\n",
      " 0.00268804 0.00431676 0.00204908 0.08000825 0.00262522 0.00193372\n",
      " 0.01993478 0.00242195 0.00404837 0.00473619 0.00478151 0.22999348\n",
      " 0.00182794 0.00445612 0.00538402 0.00342256 0.00252717 0.00430175\n",
      " 0.00251332 0.01435043 0.00226761 0.00700604 0.0025793  0.00277798\n",
      " 0.00332933 0.00249952 0.25991189 0.01348107 0.05547782 0.00222837\n",
      " 0.02895038 0.06214652 0.00365244 0.00168582 0.00294814 0.02732143\n",
      " 0.09087864 0.22135222 0.00177686 0.57347465 0.00301251 0.00416095\n",
      " 0.00197597 0.38578924 0.00153053 0.00334985 0.00191552 0.01244852\n",
      " 0.00313728 0.00239422 0.00262522 0.00168232 0.00329733 0.19652131\n",
      " 0.00455686 0.0052616  0.00970884 0.00287356 0.00430708 0.00287356\n",
      " 0.00444854 0.00180384 0.00341311 0.1466127  0.00347093 0.0096764\n",
      " 0.29050232 0.00200716 0.00275215 0.00208282 0.0025793  0.002415\n",
      " 0.00434715 0.00204908 0.00342316 0.00188216 0.00151016 0.00270701\n",
      " 0.00314203 0.0151751  0.00216966 0.0055815  0.00206445 0.00369255\n",
      " 0.00259188 0.49656442 0.00348796 0.00394108 0.00375448 0.00168278\n",
      " 0.00201519 0.00800709 0.00268804 0.00363222 0.00196798 0.00335812\n",
      " 0.00614838 0.05123503 0.00806111 0.00502588 0.0022092  0.13723177\n",
      " 0.00898764 0.002415   0.00309201 0.00453106 0.00609118]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91       145\n",
      "           1       0.14      1.00      0.24         4\n",
      "\n",
      "    accuracy                           0.83       149\n",
      "   macro avg       0.57      0.91      0.57       149\n",
      "weighted avg       0.98      0.83      0.89       149\n",
      "\n",
      "AUC-ROC for Fold 1: 0.9638\n",
      "G-Mean for Fold 1: 0.9097\n",
      "\n",
      "--- Fold 2 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 2\n",
      "Training classifier 2/3 for fold 2\n",
      "Training classifier 3/3 for fold 2\n",
      "[0.02378585 0.00523969 0.00272609 0.00265643 0.54265056 0.07532644\n",
      " 0.07627626 0.00191191 0.00238963 0.0102793  0.02753915 0.00289426\n",
      " 0.00150918 0.00665152 0.00147997 0.06315621 0.00198086 0.00570331\n",
      " 0.0025103  0.02231099 0.06257832 0.00408221 0.00447434 0.00269526\n",
      " 0.00368769 0.0020256  0.00251038 0.00159287 0.05997267 0.00240038\n",
      " 0.00163669 0.00199039 0.00676761 0.00651847 0.00164401 0.00162593\n",
      " 0.00303556 0.0013297  0.00574474 0.00700698 0.00700197 0.0020657\n",
      " 0.02429359 0.00571254 0.00162644 0.41841728 0.36464086 0.00197985\n",
      " 0.00308712 0.00896274 0.02706557 0.95820864 0.94770781 0.00159202\n",
      " 0.00570331 0.02281778 0.00275374 0.00194175 0.00276346 0.01007827\n",
      " 0.00136381 0.00240967 0.0017784  0.0014628  0.1360331  0.00255451\n",
      " 0.02502324 0.00305302 0.00266566 0.44496393 0.00136381 0.00310392\n",
      " 0.00151565 0.00159515 0.00147144 0.00140411 0.00328819 0.01571926\n",
      " 0.00261059 0.01960649 0.00136381 0.00587418 0.00300677 0.00544766\n",
      " 0.00221759 0.00139856 0.0026747  0.01370398 0.00665643 0.00671876\n",
      " 0.02193617 0.0016842  0.00149081 0.00178352 0.0021579  0.00883473\n",
      " 0.00246419 0.00395135 0.01833928 0.00528835 0.00335333 0.00244973\n",
      " 0.25899499 0.00168813 0.00952192 0.0015621  0.71687518 0.00322436\n",
      " 0.01594386 0.01036303 0.00240038 0.00238277 0.00181198 0.0018082\n",
      " 0.00400244 0.0027803  0.00579113 0.00235079 0.46524256 0.00203215\n",
      " 0.008403   0.00304843 0.00174854 0.00272098 0.00635404 0.00253847\n",
      " 0.08160218 0.00524552 0.02664289 0.00501013 0.00157627 0.00186852\n",
      " 0.05885512 0.00544532 0.00161798 0.00151147 0.00140411 0.00346251\n",
      " 0.00284424 0.00178272 0.1250308  0.00221759 0.09354932 0.00197686\n",
      " 0.00338463 0.84093436 0.00157618 0.28345601 0.23557001]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.76      0.86       145\n",
      "           1       0.10      1.00      0.19         4\n",
      "\n",
      "    accuracy                           0.77       149\n",
      "   macro avg       0.55      0.88      0.52       149\n",
      "weighted avg       0.98      0.77      0.84       149\n",
      "\n",
      "AUC-ROC for Fold 2: 0.9759\n",
      "G-Mean for Fold 2: 0.8710\n",
      "\n",
      "--- Fold 3 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 3\n",
      "Training classifier 2/3 for fold 3\n",
      "Training classifier 3/3 for fold 3\n",
      "[0.00296385 0.02144007 0.00217182 0.00396451 0.00293288 0.00716532\n",
      " 0.00252278 0.00275472 0.00515627 0.00501736 0.00208602 0.00311582\n",
      " 0.00421051 0.00183995 0.00240063 0.00172368 0.00173481 0.00153834\n",
      " 0.00166086 0.0025628  0.02900725 0.00139884 0.0041291  0.00379056\n",
      " 0.00223946 0.12580611 0.00415317 0.00174879 0.90258942 0.00173618\n",
      " 0.00579853 0.00272461 0.00173481 0.00166015 0.00290931 0.0176222\n",
      " 0.00233559 0.00168402 0.00175629 0.005919   0.00556363 0.02420135\n",
      " 0.00163915 0.02636121 0.00336855 0.00531382 0.00180303 0.00214183\n",
      " 0.0016517  0.00302564 0.00323695 0.00169613 0.00883298 0.00396973\n",
      " 0.00159273 0.00794771 0.0081284  0.00216094 0.00176719 0.00309873\n",
      " 0.00308886 0.03603561 0.00487146 0.00244022 0.01340314 0.00139964\n",
      " 0.00188693 0.00208876 0.00576752 0.00224843 0.09763755 0.00519277\n",
      " 0.01043284 0.03057739 0.0051233  0.03421939 0.00453366 0.00249114\n",
      " 0.00351257 0.00199099 0.00231075 0.00225309 0.0019681  0.00749082\n",
      " 0.00146744 0.00258788 0.00231759 0.49828921 0.39581009 0.60203419\n",
      " 0.04961746 0.00139884 0.00199057 0.00264923 0.00231759 0.002213\n",
      " 0.00176949 0.00281774 0.00189028 0.00156883 0.00273795 0.00240485\n",
      " 0.00438529 0.02233863 0.00376824 0.06311778 0.00245316 0.02787545\n",
      " 0.0074965  0.02105972 0.00214708 0.00233727 0.00294525 0.01625461\n",
      " 0.00837059 0.00173931 0.001922   0.00830678 0.00248355 0.00168092\n",
      " 0.00159273 0.0076964  0.00225309 0.00203185 0.0032702  0.00143147\n",
      " 0.00257428 0.00262325 0.00236121 0.11336044 0.00248355 0.00358616\n",
      " 0.03088504 0.00261484 0.00304232 0.00529094 0.52003033 0.00259199\n",
      " 0.00306532 0.0091958  0.01480742 0.00694965 0.0021531  0.00177255\n",
      " 0.00166015 0.00156222 0.00166086 0.00168241 0.00887317]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92       145\n",
      "           1       0.15      1.00      0.27         4\n",
      "\n",
      "    accuracy                           0.85       149\n",
      "   macro avg       0.58      0.92      0.59       149\n",
      "weighted avg       0.98      0.85      0.90       149\n",
      "\n",
      "AUC-ROC for Fold 3: 0.9966\n",
      "G-Mean for Fold 3: 0.9210\n",
      "\n",
      "--- Fold 4 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 4\n",
      "Training classifier 2/3 for fold 4\n",
      "Training classifier 3/3 for fold 4\n",
      "[0.00888244 0.00161936 0.00196787 0.00202448 0.00196685 0.00441155\n",
      " 0.00199349 0.00593679 0.01894266 0.00630035 0.00189311 0.0052305\n",
      " 0.00389588 0.00465874 0.00230693 0.00280533 0.00215692 0.0043663\n",
      " 0.00341806 0.80873187 0.00969056 0.00212441 0.00212297 0.0016181\n",
      " 0.0036645  0.00229974 0.00193523 0.00388078 0.00262901 0.00158601\n",
      " 0.00169008 0.00189311 0.00152584 0.00156741 0.00477401 0.00505763\n",
      " 0.00212931 0.00253001 0.00339688 0.00184182 0.00359477 0.00672863\n",
      " 0.00678447 0.00221534 0.00184073 0.00125312 0.00576726 0.00125312\n",
      " 0.00181925 0.00213583 0.00693857 0.01275822 0.00304077 0.00245459\n",
      " 0.00225294 0.00575487 0.00174976 0.00297773 0.00430855 0.00227063\n",
      " 0.01799842 0.00599834 0.00263204 0.00206796 0.00234744 0.00162926\n",
      " 0.00236047 0.02146812 0.24972098 0.00245849 0.00248886 0.25569979\n",
      " 0.00280533 0.00508775 0.98966159 0.91428296 0.40902006 0.00189311\n",
      " 0.00308778 0.01015409 0.00196505 0.00270486 0.00176477 0.00779904\n",
      " 0.00412745 0.00349694 0.00201503 0.00288913 0.00296319 0.00178327\n",
      " 0.0027076  0.004782   0.01224668 0.00524682 0.00551859 0.01603738\n",
      " 0.00155545 0.00320879 0.02794294 0.00528303 0.00563004 0.00391186\n",
      " 0.00280533 0.00212582 0.00203954 0.00319319 0.00490532 0.00222137\n",
      " 0.0022755  0.00444104 0.00785305 0.00322128 0.0014114  0.00169496\n",
      " 0.00197436 0.00449495 0.00264234 0.00181011 0.00258199 0.00249035\n",
      " 0.00555094 0.00205253 0.00311641 0.00774715 0.01677669 0.00189311\n",
      " 0.00630957 0.00164948 0.0021984  0.00223516 0.00169374 0.00830632\n",
      " 0.00139691 0.00177335 0.01770314 0.00326693 0.00862471 0.00164502\n",
      " 0.00305049 0.00224554 0.00518817 0.00260394 0.08896952 0.0082772\n",
      " 0.00868334 0.00359227 0.00149258 0.00277781 0.00384327]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.95       145\n",
      "           1       0.18      0.75      0.29         4\n",
      "\n",
      "    accuracy                           0.90       149\n",
      "   macro avg       0.58      0.83      0.62       149\n",
      "weighted avg       0.97      0.90      0.93       149\n",
      "\n",
      "AUC-ROC for Fold 4: 0.9466\n",
      "G-Mean for Fold 4: 0.8232\n",
      "\n",
      "--- Fold 5 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 5\n",
      "Training classifier 2/3 for fold 5\n",
      "Training classifier 3/3 for fold 5\n",
      "[0.00290595 0.0029666  0.05681703 0.0022886  0.00203267 0.00258879\n",
      " 0.00137576 0.00272056 0.00465573 0.00140341 0.00288327 0.0020305\n",
      " 0.00849511 0.00184034 0.00542772 0.00644274 0.02266489 0.00262499\n",
      " 0.00190374 0.00600773 0.00707265 0.0024035  0.00210861 0.00210877\n",
      " 0.00229257 0.0063478  0.00583273 0.00253529 0.00362012 0.00268792\n",
      " 0.00286553 0.00518008 0.00134389 0.00395876 0.00268785 0.00326097\n",
      " 0.00441141 0.51113369 0.00369587 0.00501287 0.00158314 0.02063972\n",
      " 0.00182427 0.05118725 0.00212934 0.00200231 0.00183764 0.0024909\n",
      " 0.00487314 0.00317541 0.00203732 0.00203732 0.00127352 0.00169174\n",
      " 0.00137139 0.00347338 0.02377668 0.00225645 0.20506639 0.00248607\n",
      " 0.00216525 0.00201472 0.0063478  0.00670838 0.00163688 0.00436669\n",
      " 0.00364428 0.00281383 0.01081573 0.00240439 0.00929963 0.52179197\n",
      " 0.02259529 0.12242487 0.00191724 0.00235125 0.1957647  0.0019047\n",
      " 0.00351881 0.00267074 0.00397492 0.00158515 0.00260891 0.95443134\n",
      " 0.00122858 0.00166668 0.00941288 0.15901259 0.00263088 0.0012588\n",
      " 0.00626989 0.00183241 0.00332826 0.01004153 0.01678518 0.00268355\n",
      " 0.00138117 0.02540734 0.00775996 0.01494075 0.00222651 0.01009673\n",
      " 0.01342533 0.00155445 0.00311556 0.71046235 0.00182014 0.00187296\n",
      " 0.00212282 0.00271474 0.00498499 0.00187971 0.00271133 0.01534948\n",
      " 0.00278481 0.00863872 0.00396368 0.00400138 0.01077071 0.00248904\n",
      " 0.00139475 0.00358939 0.00254753 0.00242158 0.00244523 0.00180795\n",
      " 0.00201923 0.0056553  0.00305303 0.00228197 0.00286442 0.01987288\n",
      " 0.0059389  0.0013608  0.00198273 0.00278393 0.01154577 0.00176718\n",
      " 0.02051191 0.00895505 0.00129654 0.00123544 0.03121974 0.0152345\n",
      " 0.0687701  0.00492037 0.00662684 0.03050318]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.81      0.89       145\n",
      "           1       0.07      0.67      0.12         3\n",
      "\n",
      "    accuracy                           0.80       148\n",
      "   macro avg       0.53      0.74      0.51       148\n",
      "weighted avg       0.97      0.80      0.87       148\n",
      "\n",
      "AUC-ROC for Fold 5: 0.9287\n",
      "G-Mean for Fold 5: 0.7334\n",
      "\n",
      "--- Fold 6 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 6\n",
      "Training classifier 2/3 for fold 6\n",
      "Training classifier 3/3 for fold 6\n",
      "[0.01321925 0.00145034 0.00210272 0.00228724 0.05766573 0.68613301\n",
      " 0.00185864 0.82506315 0.0033392  0.00202058 0.00297911 0.0014288\n",
      " 0.00221878 0.00474314 0.02799916 0.00234031 0.00365032 0.00160647\n",
      " 0.00388394 0.00218298 0.00128438 0.00140857 0.00194073 0.0049922\n",
      " 0.01330059 0.0024109  0.01337315 0.00338722 0.00328005 0.01044983\n",
      " 0.01448112 0.02150358 0.04396651 0.00242618 0.00223682 0.00145034\n",
      " 0.01048239 0.00208643 0.05181636 0.0020762  0.0038889  0.01072017\n",
      " 0.00233186 0.02263627 0.00136532 0.04551074 0.00289761 0.00215093\n",
      " 0.00773984 0.00507882 0.00243855 0.00417759 0.0112475  0.00191721\n",
      " 0.00395403 0.00145502 0.00319572 0.00174627 0.00286395 0.00223405\n",
      " 0.00801668 0.00375949 0.00162927 0.00280907 0.24315183 0.00130274\n",
      " 0.00205181 0.00235272 0.00212558 0.00382411 0.01092003 0.00705392\n",
      " 0.00173453 0.00153655 0.00601707 0.01932235 0.00205181 0.0032622\n",
      " 0.00210272 0.00185003 0.00495924 0.00306495 0.00140355 0.00378691\n",
      " 0.00594075 0.00769208 0.00421745 0.00377341 0.00157325 0.00145502\n",
      " 0.00162969 0.00224078 0.00184697 0.03170883 0.00202058 0.00195413\n",
      " 0.01295352 0.00202061 0.00205181 0.00198214 0.02951755 0.00199646\n",
      " 0.03398421 0.00162407 0.00150814 0.00242611 0.00306301 0.00378787\n",
      " 0.00488442 0.00417776 0.00339223 0.00478698 0.00799561 0.96104199\n",
      " 0.00279057 0.00185757 0.00159765 0.00197255 0.59384418 0.00160647\n",
      " 0.00240881 0.00129864 0.00193143 0.00313751 0.00173546 0.01802564\n",
      " 0.00250926 0.0100373  0.00134282 0.00172184 0.01212604 0.00205181\n",
      " 0.0012819  0.18107152 0.02267657 0.09954909 0.10176047 0.00211937\n",
      " 0.00177375 0.00145502 0.00130792 0.00197791 0.42100407 0.01157398\n",
      " 0.00143241 0.00112948 0.02247259 0.00145502]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.86       145\n",
      "           1       0.06      0.67      0.10         3\n",
      "\n",
      "    accuracy                           0.76       148\n",
      "   macro avg       0.52      0.72      0.48       148\n",
      "weighted avg       0.97      0.76      0.85       148\n",
      "\n",
      "AUC-ROC for Fold 6: 0.9149\n",
      "G-Mean for Fold 6: 0.7144\n",
      "\n",
      "--- Fold 7 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 7\n",
      "Training classifier 2/3 for fold 7\n",
      "Training classifier 3/3 for fold 7\n",
      "[0.00437888 0.00149382 0.00150594 0.0047665  0.00122608 0.38248503\n",
      " 0.00600571 0.0033759  0.00150594 0.53917218 0.03577775 0.01192513\n",
      " 0.00206986 0.00213396 0.00270924 0.00580792 0.0028755  0.01031245\n",
      " 0.00213818 0.00256151 0.0016547  0.00494382 0.00260986 0.01617389\n",
      " 0.0034728  0.00524611 0.00292685 0.00594285 0.04812027 0.00555635\n",
      " 0.00254784 0.00220364 0.00220364 0.00220364 0.00242641 0.00212479\n",
      " 0.03028508 0.02688332 0.00752487 0.02278382 0.00269569 0.00154978\n",
      " 0.04687307 0.00599618 0.00220364 0.00205236 0.0016547  0.00214043\n",
      " 0.00170695 0.00360695 0.01085572 0.0018857  0.00755601 0.00277326\n",
      " 0.00162478 0.0013876  0.0024421  0.01847084 0.0062519  0.00214821\n",
      " 0.0045927  0.00220364 0.01245722 0.00255504 0.01152786 0.00669422\n",
      " 0.00166439 0.00351091 0.00436677 0.00122608 0.00152    0.00221386\n",
      " 0.00551427 0.00334035 0.00215906 0.00223504 0.00225764 0.00523231\n",
      " 0.00125246 0.00190585 0.00216686 0.00933602 0.00682027 0.00207787\n",
      " 0.00195768 0.00203256 0.0035541  0.0039939  0.00201162 0.00554632\n",
      " 0.00189114 0.00440979 0.00773203 0.00244369 0.00209821 0.00161747\n",
      " 0.01195245 0.11705809 0.00646447 0.01655122 0.00357801 0.00469674\n",
      " 0.00325676 0.0034918  0.00238736 0.00325135 0.82158542 0.00674898\n",
      " 0.00146506 0.01102947 0.6689201  0.0016547  0.01542119 0.00175821\n",
      " 0.00244281 0.01621517 0.00234106 0.00693242 0.00259255 0.00467718\n",
      " 0.00144645 0.00146506 0.00146506 0.0022398  0.03359695 0.00155599\n",
      " 0.00192599 0.00603976 0.00234106 0.0024421  0.00269785 0.0048507\n",
      " 0.00321686 0.00182362 0.00154978 0.00620401 0.00667418 0.00178942\n",
      " 0.00136747 0.0017328  0.02585411 0.00276788 0.05483197 0.00297219\n",
      " 0.00154978 0.01714624 0.00292685 0.00327839]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91       145\n",
      "           1       0.11      1.00      0.20         3\n",
      "\n",
      "    accuracy                           0.84       148\n",
      "   macro avg       0.56      0.92      0.55       148\n",
      "weighted avg       0.98      0.84      0.90       148\n",
      "\n",
      "AUC-ROC for Fold 7: 0.9839\n",
      "G-Mean for Fold 7: 0.9135\n",
      "\n",
      "--- Fold 8 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 8\n",
      "Training classifier 2/3 for fold 8\n",
      "Training classifier 3/3 for fold 8\n",
      "[0.00245681 0.00193473 0.00231479 0.00145387 0.00862166 0.01963131\n",
      " 0.00983125 0.00729804 0.00208255 0.00181595 0.0077681  0.00386287\n",
      " 0.01108062 0.0021223  0.02060217 0.00186936 0.00198017 0.00216082\n",
      " 0.03781327 0.00164203 0.00212306 0.00713312 0.00576991 0.00196166\n",
      " 0.01347942 0.00211756 0.0017773  0.00180086 0.00264075 0.00395376\n",
      " 0.0023375  0.00338049 0.00176252 0.00138508 0.0059142  0.00246725\n",
      " 0.00182196 0.00145742 0.00613574 0.00127535 0.00253    0.00304967\n",
      " 0.00209839 0.00878483 0.0030863  0.00231479 0.02904022 0.00143414\n",
      " 0.00176013 0.00232211 0.00324352 0.01569815 0.00204044 0.00125909\n",
      " 0.00256617 0.00271005 0.00351566 0.68644387 0.00111829 0.00160918\n",
      " 0.00174535 0.00267134 0.02449421 0.00392413 0.00302366 0.00273965\n",
      " 0.00220392 0.00276134 0.00167863 0.00316835 0.00136329 0.00197833\n",
      " 0.00400373 0.00123994 0.00559103 0.00543131 0.00164419 0.00188387\n",
      " 0.00112959 0.00185349 0.00168764 0.01455471 0.01106961 0.00450216\n",
      " 0.00461236 0.00197322 0.00486682 0.00219452 0.00180086 0.00172559\n",
      " 0.00772177 0.00319321 0.00267974 0.00358821 0.00453983 0.0023075\n",
      " 0.0043227  0.02061076 0.00238132 0.02179289 0.00558054 0.00382053\n",
      " 0.00260227 0.00136329 0.00124333 0.00152467 0.00192025 0.00281239\n",
      " 0.03554068 0.00208624 0.03435699 0.00139111 0.01018283 0.00159463\n",
      " 0.87974413 0.00252221 0.00462628 0.00157697 0.00299293 0.03736679\n",
      " 0.00197322 0.00219828 0.00506483 0.00231479 0.00192599 0.0042256\n",
      " 0.00242076 0.07700535 0.00161579 0.00357113 0.01635746 0.00391345\n",
      " 0.00156156 0.00184098 0.00164171 0.00434313 0.0013238  0.00177131\n",
      " 0.03304724 0.00134591 0.00282798 0.00185861 0.00141767 0.00195076\n",
      " 0.00849027 0.01448008 0.00229388 0.00184347]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92       145\n",
      "           1       0.09      0.67      0.16         3\n",
      "\n",
      "    accuracy                           0.86       148\n",
      "   macro avg       0.54      0.76      0.54       148\n",
      "weighted avg       0.97      0.86      0.91       148\n",
      "\n",
      "AUC-ROC for Fold 8: 0.9310\n",
      "G-Mean for Fold 8: 0.7581\n",
      "\n",
      "--- Fold 9 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 9\n",
      "Training classifier 2/3 for fold 9\n",
      "Training classifier 3/3 for fold 9\n",
      "[0.00566089 0.0165724  0.00422858 0.00850896 0.00511131 0.00704012\n",
      " 0.00200018 0.00189814 0.40985656 0.00583882 0.00627694 0.00149954\n",
      " 0.00902508 0.00518897 0.00435465 0.00166852 0.00143453 0.00159807\n",
      " 0.00209521 0.00160124 0.00204203 0.02147216 0.00157655 0.00112886\n",
      " 0.1185215  0.00150479 0.00145714 0.00560942 0.00241634 0.040873\n",
      " 0.00165807 0.00199325 0.00171508 0.00164038 0.00293933 0.01140805\n",
      " 0.00125846 0.00473388 0.00133519 0.00465952 0.00364155 0.00584977\n",
      " 0.00153238 0.00252576 0.00206934 0.00225924 0.01328952 0.00156692\n",
      " 0.0043054  0.00182106 0.01741878 0.00138186 0.00204203 0.00719965\n",
      " 0.00258992 0.00143113 0.00333365 0.00210425 0.00130549 0.00150901\n",
      " 0.00128851 0.01525649 0.01187114 0.00394245 0.00204203 0.00143453\n",
      " 0.00211103 0.00249983 0.00146629 0.00245622 0.00754766 0.00300796\n",
      " 0.00211554 0.00222879 0.00246755 0.00682223 0.00547652 0.00939524\n",
      " 0.00198764 0.00336495 0.00237682 0.00342263 0.02534385 0.00768782\n",
      " 0.00674327 0.00204203 0.00422858 0.00154015 0.00805793 0.00150051\n",
      " 0.0053386  0.00294437 0.00848766 0.00480772 0.01026375 0.00217818\n",
      " 0.00535218 0.00245217 0.00157655 0.00128129 0.0132282  0.00476506\n",
      " 0.00151081 0.00165979 0.0029032  0.0020881  0.01258794 0.00224468\n",
      " 0.00160046 0.00273406 0.00746919 0.00472697 0.00900205 0.00150154\n",
      " 0.00220552 0.00205832 0.02434045 0.00239011 0.00157727 0.00596958\n",
      " 0.00148864 0.00235898 0.00421748 0.00428175 0.00148324 0.00143547\n",
      " 0.01374627 0.24054558 0.00150051 0.00298127 0.00155999 0.0016724\n",
      " 0.00143453 0.00235873 0.00223787 0.00709137 0.00205832 0.00128129\n",
      " 0.00145714 0.00112886 0.00270496 0.00347999 0.28342218 0.00937362\n",
      " 0.00162649 0.00148324 0.08314411 0.00256352]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93       145\n",
      "           1       0.05      0.33      0.09         3\n",
      "\n",
      "    accuracy                           0.86       148\n",
      "   macro avg       0.52      0.60      0.51       148\n",
      "weighted avg       0.97      0.86      0.91       148\n",
      "\n",
      "AUC-ROC for Fold 9: 0.8506\n",
      "G-Mean for Fold 9: 0.5403\n",
      "\n",
      "--- Fold 10 ---\n",
      "ÂºÄÂßãËøáÈááÊ†∑\n",
      "Training classifier 1/3 for fold 10\n",
      "Training classifier 2/3 for fold 10\n",
      "Training classifier 3/3 for fold 10\n",
      "[0.00237984 0.00143832 0.00228638 0.00444934 0.72364509 0.01209626\n",
      " 0.00180784 0.00142015 0.00145863 0.00182985 0.00234695 0.00756963\n",
      " 0.00177321 0.00374762 0.0020458  0.0022592  0.00936946 0.00169935\n",
      " 0.00237984 0.00232717 0.00259599 0.01072722 0.00412023 0.00126056\n",
      " 0.86741867 0.00284283 0.00137016 0.02153431 0.63087732 0.8957874\n",
      " 0.00865271 0.03334936 0.42276126 0.00278011 0.00221874 0.04944644\n",
      " 0.0031741  0.00273367 0.00148457 0.00192764 0.00321909 0.00244046\n",
      " 0.00190291 0.00435039 0.00158051 0.0026108  0.00533337 0.00378777\n",
      " 0.0018267  0.00178888 0.00402656 0.00275666 0.03560701 0.91439372\n",
      " 0.00283726 0.97563098 0.00334864 0.00275666 0.00157409 0.00267475\n",
      " 0.00275308 0.00215932 0.00275745 0.00371297 0.00237984 0.00370963\n",
      " 0.00476298 0.00280797 0.00373317 0.00992729 0.00516206 0.00192991\n",
      " 0.00210369 0.00583546 0.0046412  0.01017915 0.00731521 0.03571215\n",
      " 0.00127173 0.01602805 0.00251716 0.00159446 0.03037132 0.09086847\n",
      " 0.00374762 0.0037088  0.00807382 0.00237984 0.00228638 0.00268072\n",
      " 0.0018267  0.00798094 0.00177909 0.00603889 0.00308276 0.0027181\n",
      " 0.00245282 0.00691766 0.00201282 0.02907314 0.00444993 0.02087474\n",
      " 0.00346956 0.00465116 0.00457095 0.00204211 0.00345745 0.0027181\n",
      " 0.0043006  0.00191255 0.00373852 0.00273956 0.00310826 0.00334824\n",
      " 0.00163161 0.00705302 0.01811688 0.05290718 0.00378777 0.00154511\n",
      " 0.00218674 0.00206101 0.00223951 0.00377341 0.00244046 0.0018267\n",
      " 0.00227066 0.00230408 0.09113901 0.01673925 0.00763188 0.002784\n",
      " 0.00248266 0.00431614 0.01306894 0.00353185 0.00233526 0.00189557\n",
      " 0.00173805 0.14407018 0.00416936 0.0140887  0.00814018 0.00996689\n",
      " 0.00164355 0.00413588 0.008243   0.01052818]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91       144\n",
      "           1       0.14      1.00      0.25         4\n",
      "\n",
      "    accuracy                           0.84       148\n",
      "   macro avg       0.57      0.92      0.58       148\n",
      "weighted avg       0.98      0.84      0.89       148\n",
      "\n",
      "AUC-ROC for Fold 10: 0.9948\n",
      "G-Mean for Fold 10: 0.9129\n",
      "\n",
      "Average AUC-ROC over 10 folds: 0.9487\n",
      "\n",
      "Average G-Mean over 10 folds: 0.8098\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from dataset.load_dat import load_keel_dat\n",
    "from kmeans_smote import KMeansSMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import numpy as np\n",
    "from loss import *\n",
    "from model import FocalXGBClassifier\n",
    "from optimize import *\n",
    "print(xgb.train)\n",
    "def compute_gmean(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape != (2, 2):\n",
    "        print(\"Warning: confusion matrix is not binary, cannot compute G-Mean.\")\n",
    "        return 0.0\n",
    "\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    gmean = np.sqrt(sensitivity * specificity)\n",
    "    return gmean\n",
    "# **EasyEnsemble ÂÆûÁé∞**\n",
    "def easy_ensemble(X, y, n_estimators=10):\n",
    "    \"\"\"\n",
    "    EasyEnsemble: ÂØπÂ§öÊï∞Á±ªËøõË°åÂ§öÊ¨°Ê¨†ÈááÊ†∑ÔºåÁîüÊàêÂ§ö‰∏™Â≠êÊï∞ÊçÆÈõÜ„ÄÇ\n",
    "    Args:\n",
    "        X: ÁâπÂæÅÁü©Èòµ\n",
    "        y: Ê†áÁ≠æ\n",
    "        n_estimators: Ê¨≤ÁîüÊàêÁöÑÂº±ÂàÜÁ±ªÂô®Êï∞ÈáèÔºàÊ¨†ÈááÊ†∑Ê¨°Êï∞Ôºâ\n",
    "    Returns:\n",
    "        samples: List of (X_resampled, y_resampled)\n",
    "    \"\"\"\n",
    "    kmeans_smote = KMeansSMOTE(\n",
    "    kmeans_args={\n",
    "        'n_clusters': 100\n",
    "    },\n",
    "    smote_args={\n",
    "        'k_neighbors': 10\n",
    "    }\n",
    "    )\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    samples = []\n",
    "    print('ÂºÄÂßãËøáÈááÊ†∑')\n",
    "    for _ in range(n_estimators):\n",
    "        #X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "        X_resampled, y_resampled = kmeans_smote.fit_resample(X, y)\n",
    "        samples.append((X_resampled, y_resampled))\n",
    "    return samples\n",
    "\n",
    "# **Âä†ËΩΩÊï∞ÊçÆÈõÜÔºà‰ª• glass1 Êï∞ÊçÆÈõÜ‰∏∫‰æãÔºâ**\n",
    "def load_keel(name):\n",
    "    # ‰ªé UCI ÊàñÊú¨Âú∞Âä†ËΩΩ 'glass1' Êï∞ÊçÆÈõÜ\n",
    "    # ‰∏ãËΩΩÂú∞ÂùÄ: https://sci2s.ugr.es/keel/dataset/data/classification/glass1.zip\n",
    "    # ÂÅáËÆæÊï∞ÊçÆÂ∑≤Áªè‰øùÂ≠ò‰∏∫ glass1.csv\n",
    "    data = load_keel_dat(\"dataset/\"+name)  # ÊõøÊç¢‰∏∫ÂÆûÈôÖË∑ØÂæÑ\n",
    "    print(data)\n",
    "    X = data.iloc[:, :-1].values  # ÁâπÂæÅ\n",
    "    y = data.iloc[:, -1].values   # Ê†áÁ≠æ\n",
    "    return X, y\n",
    "\n",
    "# ‰∏ªÊµÅÁ®ãÔºàK ÊäòÔºâ\n",
    "def main():\n",
    "    #name = \"abalone19.dat\" #scale_pos_weight=0.1\n",
    "    name = \"yeast6.dat\"\n",
    "    X, y = load_keel(name)\n",
    "    print(X.shape)\n",
    "    print(y)\n",
    "    #X, y, selected_feature_names = mrmr_with_combinations(X, y, keep_ratio=0.8)\n",
    "    #print(\"ÈÄâ‰∏≠ÁâπÂæÅÂêçÔºö\", selected_feature_names)\n",
    "    # print(\"Á≠õÈÄâ‰πãÂêéÁöÑÁâπÂæÅ\")\n",
    "    # print(X.shape)\n",
    "    # print(y)\n",
    "    if ' positive' in y:\n",
    "        y = (y == ' positive').astype(int)  # Áªü‰∏ÄÊ†áÁ≠æ‰∏∫ 0 Âíå 1\n",
    "    print(\"üîç Ê≠£Âú®ÊâßË°åÂÖ®Â±ÄË∂ÖÂèÇÊï∞ÊêúÁ¥¢Ôºà‰ΩøÁî®ËÆ≠ÁªÉÈõÜÔºâ...\")\n",
    "    # ‰ΩøÁî®Êï¥‰∏™Êï∞ÊçÆÈõÜË∞ÉÂèÇÔºàÊàñÊüê‰∏ÄÈÉ®ÂàÜÔºâ\n",
    "    X_subtrain, X_val, y_subtrain, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    best_params = optimize_xgb_multiobjective(X_subtrain, y_subtrain, X_val, y_val, n_trials=30)\n",
    "    print(\"‚úÖ ÂÖ®Â±ÄÊúÄ‰ºòÂèÇÊï∞Ôºö\", best_params)\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    all_auc = []\n",
    "    all_gmean = []\n",
    "    First = True\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "        # EasyEnsemble\n",
    "        n_estimators = 3\n",
    "        samples = easy_ensemble(X_train, y_train, n_estimators=n_estimators)\n",
    "\n",
    "        classifiers = []\n",
    "        for i, (X_resampled, y_resampled) in enumerate(samples):\n",
    "            print(f\"Training classifier {i+1}/{n_estimators} for fold {fold}\")\n",
    "            # model = XGBClassifier(\n",
    "            #     n_estimators=100,\n",
    "            #     max_depth=6,\n",
    "            #     learning_rate=0.1,\n",
    "            #     subsample=0.8,\n",
    "            #     colsample_bytree=0.8,\n",
    "            #     use_label_encoder=False,\n",
    "            #     random_state=42\n",
    "            # )\n",
    "\n",
    "            # # fit with custom objective\n",
    "            # model.fit(\n",
    "            #     X_resampled, y_resampled,\n",
    "            # )\n",
    "            # model = FocalXGBClassifier(num_boost_round=100, alpha=0.75, gamma=2.0)\n",
    "            # model.fit(X_resampled, y_resampled ,eval_set=[(X_resampled, y_resampled)],verbose_eval=True)\n",
    "            \n",
    "\n",
    "            #if First:\n",
    "            #    best_params = optimize_xgb_multiobjective(X_resampled, y_resampled, X_test, y_test, n_trials=30)\n",
    "            #    First = False\n",
    "\n",
    "            model = XGBClassifier(**best_params)\n",
    "            model.fit(\n",
    "                X_resampled, y_resampled,\n",
    "            )\n",
    "            classifiers.append(model)\n",
    "\n",
    "        # ÊµãËØïÈõÜÈ¢ÑÊµã\n",
    "        ensemble_preds = np.zeros_like(y_test, dtype=float)\n",
    "        for model in classifiers:\n",
    "            ensemble_preds += model.predict_proba(X_test)[:, 1]\n",
    "        ensemble_preds /= n_estimators\n",
    "\n",
    "        print(ensemble_preds)\n",
    "        y_pred = (ensemble_preds >= 0.01).astype(int)\n",
    "        auc = roc_auc_score(y_test, ensemble_preds)\n",
    "        all_auc.append(auc)\n",
    "\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(f\"AUC-ROC for Fold {fold}: {auc:.4f}\")\n",
    "        gmean = compute_gmean(y_test, y_pred)\n",
    "        all_gmean.append(gmean)\n",
    "        print(f\"G-Mean for Fold {fold}: {gmean:.4f}\")\n",
    "\n",
    "    # ÊâÄÊúâÊäòÁöÑÂπ≥Âùá AUC\n",
    "    print(f\"\\nAverage AUC-ROC over {skf.n_splits} folds: {np.mean(all_auc):.4f}\")\n",
    "    print(f\"\\nAverage G-Mean over {skf.n_splits} folds: {np.mean(all_gmean):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhy_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
